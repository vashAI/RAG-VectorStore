{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following packages are needed for this project \n",
    "# --------------------------------------------------------------------------------------------------------------\n",
    "# langchain_community.document_loaders:  Contains PyPDFLoader which is essential for \n",
    "#                                        loading and processing PDF documents.\n",
    "# --------------------------------------------------------------------------------------------------------------\n",
    "# langchain.text_splitter:               Provides RecursiveCharacterTextSplitter, useful for \n",
    "#                                        splitting text into manageable chunks for processing.\n",
    "# --------------------------------------------------------------------------------------------------------------\n",
    "# langchain_community.vectorstores:      Includes FAISS, a library for efficient similarity \n",
    "#                                        search and clustering of dense vectors, necessary \n",
    "#                                        for building vector stores.\n",
    "# --------------------------------------------------------------------------------------------------------------\n",
    "# langchain_openai:                      Provides OpenAIEmbeddings for generating embeddings \n",
    "#                                        using OpenAI's models, useful in vector search and \n",
    "#                                        retrieval tasks.\n",
    "# --------------------------------------------------------------------------------------------------------------\n",
    "# langchain.chains:                      Contains RetrievalQA, a module for creating question \n",
    "#                                        and answer chains based on retrieval methods.\n",
    "# --------------------------------------------------------------------------------------------------------------\n",
    "# langchain.prompts:                     Includes ChatPromptTemplate, useful for structuring \n",
    "#                                        and formatting prompts for chat models.\n",
    "# --------------------------------------------------------------------------------------------------------------\n",
    "# langchain_core.output_parsers:         Provides StrOutputParser, necessary for parsing \n",
    "#                                        string outputs from various operations.\n",
    "# --------------------------------------------------------------------------------------------------------------\n",
    "# langchain_core.runnables:              Contains RunnablePassthrough, a utility for handling \n",
    "#                                        pass-through operations in workflows.\n",
    "# --------------------------------------------------------------------------------------------------------------\n",
    "!pip install langchain_community langchain langchain_openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/christopheatten/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Importing the previously presented packages\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_api_key = 'ADD YOUR KEY'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracts and splits text from a PDF file into chunks of specified size and overlap.\n",
    "def extract_splits_from_pdf(pdf_path):\n",
    "    loader = PyPDFLoader(\"documents/Article-1.pdf\")\n",
    "    docs = loader.load()\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "    splits = text_splitter.split_documents(docs)   \n",
    "    return splits\n",
    "\n",
    "# Processes all PDF files in a folder, extracting and splitting text from each.\n",
    "def process_pdfs(pdf_folder):\n",
    "    all_splits = []\n",
    "    for filename in os.listdir(pdf_folder):\n",
    "        if filename.endswith(\".pdf\"):\n",
    "            pdf_path = os.path.join(pdf_folder, filename)\n",
    "            single_doc_splits = extract_splits_from_pdf(pdf_path)\n",
    "            all_splits.append(single_doc_splits)\n",
    "    all_splits_flattened = [item for sublist in all_splits for item in sublist]\n",
    "    return all_splits_flattened\n",
    "\n",
    "# Stores text splits in a FAISS vector store using OpenAI embeddings.\n",
    "def store_splits_in_faiss(splits, openai_api_key):\n",
    "    DB_FAISS_PATH = 'vectorstore/db_faiss'\n",
    "    embedder = OpenAIEmbeddings(api_key = openai_api_key)\n",
    "    vectorstore = FAISS.from_documents(documents = splits, embedding = embedder)\n",
    "    vectorstore.save_local(DB_FAISS_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_folder = \"documents/\" \n",
    "all_splits = process_pdfs(pdf_folder)\n",
    "store_splits_in_faiss(all_splits, openai_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads the knowledge base from a local FAISS vector store using OpenAI embeddings.\n",
    "def load_knowledgeBase(openai_api_key):\n",
    "    embeddings=OpenAIEmbeddings(api_key = openai_api_key)\n",
    "    DB_FAISS_PATH = 'vectorstore/db_faiss'\n",
    "    db = FAISS.load_local(DB_FAISS_PATH, embeddings, allow_dangerous_deserialization=True)\n",
    "    return db\n",
    "\n",
    "#  Loads the OpenAI language model (GPT-3.5-turbo) with specified parameters.\n",
    "def load_llm(openai_api_key):\n",
    "        from langchain_openai import ChatOpenAI\n",
    "        llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", \n",
    "                         temperature=0, max_tokens=500,\n",
    "                         api_key = openai_api_key)\n",
    "        return llm\n",
    "\n",
    "# Creates a prompt template for answering questions based on provided context.\n",
    "def load_prompt():\n",
    "        prompt = \"\"\" You need to answer the question in the sentence as same as in the  pdf content. \n",
    "        Given below is the context and question of the user.\n",
    "        context = {context}\n",
    "        question = {question}\n",
    "        if the answer is not in the pdf answer \"I am sorry but based on the provided information I can't answer you your question.\"\n",
    "         \"\"\"\n",
    "        prompt = ChatPromptTemplate.from_template(prompt)\n",
    "        return prompt\n",
    "\n",
    "# Formats the document content by joining page contents with newline separators.\n",
    "def format_docs(docs):\n",
    "        return \"\\n\\n\".join(doc.page_content for doc in docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "knowledgeBase = load_knowledgeBase(openai_api_key)\n",
    "llm = load_llm(openai_api_key)\n",
    "prompt = load_prompt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Who is Christophe Atten?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Searches the knowledge base for documents similar to the query using the similarity \n",
    "# search method.\n",
    "similar_embeddings = knowledgeBase.similarity_search(query)\n",
    "\n",
    "# Converts the retrieved similar documents into embeddings using OpenAI embeddings to \n",
    "# ensure they are ready for further processing or storage.\n",
    "similar_embeddings = FAISS.from_documents(documents=similar_embeddings, embedding=OpenAIEmbeddings(api_key=openai_api_key))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converts the similar embeddings into a retriever object to facilitate efficient \n",
    "# querying and retrieval of relevant documents.\n",
    "retriever = similar_embeddings.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am sorry but based on the provided information I can't answer your question.\n"
     ]
    }
   ],
   "source": [
    "# Creates a Retrieval-Augmented Generation (RAG) chain by defining the data flow:\n",
    "# - \"context\" is retrieved and formatted from the retriever, \n",
    "# - \"question\" is passed through as-is,\n",
    "# - The prompt template is applied,\n",
    "# - The LLM generates a response,\n",
    "# - The output is parsed into a string format.\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Invokes the RAG chain with the query to generate a response.\n",
    "response = rag_chain.invoke(query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
